# 4. Request深入

## 发送post请求

用法：

> response = [requests.post('https://www.baidu.com',data=data,headers=headers)](https://links.jianshu.com/go?to=http%3A%2F%2Frequests.post('https%3A%2F%2Fwww.baidu.com'%2Cdata%3Ddata%2Cheaders%3Dheaders))
>  post时不仅需要地址，还需要我们post的数据，该数据就放在data中
>  **data的形式：字典**

## 使用代理

![img](https:////upload-images.jianshu.io/upload_images/17476306-2164be5a387988ec?imageMogr2/auto-orient/strip|imageView2/2/w/741/format/webp)**正向代理与反向代理**

![img](https:////upload-images.jianshu.io/upload_images/17476306-0975975fc56c8c15?imageMogr2/auto-orient/strip|imageView2/2/w/756/format/webp)


**反向代理**：浏览器不知道服务器的地址，比如以上的图例中，浏览器知道的只是nginx服务器，因此，及时有攻击，也只能攻击nginx，不能攻击到我们的服务器

**正向代理**：浏览器知道服务器的地址

**爬虫为什么要使用代理**

- 让服务器以为不是同一个客户端在请求
- 防止我们的真实地址被泄漏，防止被追究

**使用代理**

> 用法：requests.get('[http://www.baidu.com',proxies=proxies](https://links.jianshu.com/go?to=http%3A%2F%2Fwww.baidu.com'%2Cproxies%3Dproxies))
>  proxies的形式是字典
>
> ```python
> proxies={
>    'htttp':'http://12.34.56.78:8888', #如果请求的是http
>    'https':'https://12.34.56.78:8888' #如果请求的是https的地址
> }
> ```

免费代理的网址：[https://proxy.mimvp.com/free.php](https://links.jianshu.com/go?to=https%3A%2F%2Fproxy.mimvp.com%2Ffree.php)

代理一般可以分为3种：

- 透明代理
- 普匿代理，透明以及普匿，对方是可以追查到我们的真实ip的
- 高匿代理

要注意，不是所有的ip都支持发送https的请求，有些也不支持发送post请求

代码示例：



```python
"""
0. 准备大量ip地址，组成ip池，随机选择一个ip地址来使用
    - 如何随机选择ip
        - {'ip':ip,'times':0}
        - [{},{},...{}],对这个ip的列表按照使用次数进行排序
        选择使用次数较少的几个ip，从中随机选择一个
1. 检查代理的可用性
    - 使用request添加超时参数，判断ip的质量
    - 在线代理ip质量检测的网站
"""
import requests

proxies = {"http":'http://123.56.74.13:8080'}

headers = {
'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1'
}

r = requests.get('http://www.baidu.com', proxies=proxies,headers=headers)

print(r.status_code)
print(r.request.url)
```

## session和cookie的使用与处理

**cookie和session的区别**

- cookie存储在客户的浏览器上，session存储在服务器上
- cookie的安全性不如session，别人可以分析存放在本地的cokie并进行cookie欺骗
- session会在一定时间内保存在服务器上，当访问增多，会比较比较占用服务器的性能
- cookie保存的数据容量有限（通常是4k），很多浏览器限制一个站点最多保存20个cookie

**爬虫处理cookie和session**

- 带上 cookie和session的好处：

  `能够请求到登录之后的页面`

- 带上cookie和session的弊端：

  一套cookie和session往往和一个用户对应，请求太快、次数太多，容易被服务器识别为爬虫

```
不需要cookie的时候尽量不去使用cookie
```

但是为了获取登录之后的页面，我们必须发送带有cookies的请求

携带cookie请求：

- 携带一堆cookie进行请求，把cookie组成cookie池

**如何使用**

requests提供了一个叫做session的类，来实现客户端和服务器端的会话**保持**

> 1. 实例化一个session对象：session = requests.session()
> 2. 让session发送get或post请求：r = sessioon.get(url=url,data=post_data, headers=headers)

请求登录之后的网站的思路：

- 实例化session
- 先使用session发送请求，登陆对应网站，把cookie保存在session中，`这里请求时，url应是表单的action的值，如果没有action项，就尝试抓包，看看当我们提交的时候，究竟给哪个网址发送了post请求；post_data是表单中的要提交的数据，其键为name` 
- 再使用session请求登录之后才能访问的网站，session能够自动的携带登录成功时保存在其中的cookie，进行请求

案例：访问淘宝的登录后的页面

```python
import requests

sesssion = requests.session()

headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:68.0) Gecko/20100101 Firefox/68.0'}
#注意：在copy User-Agent时，一定要复制全，不能直接在查看的时候copy，容易带上省略号
post_url = 'https://login.m.taobao.com/login.htm'
#post的url一般是在源码中表单的action中找

post_data = {
    'TPL_username':'xxxx',
    'TPL_password2':'xxxx'
}#表单中要填写的项

sesssion.post(url=post_url, data=post_data, headers=headers)

r = sesssion.get('https://h5.m.taobao.com/mlapp/mytaobao.html',headers=headers)

with open('taobao.html', 'w', encoding='utf-8') as f:
    f.write(r.content.decode()) 
#会发现taobao.html中的代码与我们登录淘宝后的https://h5.m.taobao.com/mlapp/mytaobao.html的代码一样，即成功访问了登录淘宝后的页面
```

**不发送post请求，使用cookie获取登录后的页面**

即：直接将cookie加在headers里面，而不必使用session进行post

如：

```python
import requests

headers = {
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36',
    'Cookie':'xxxx'
}
url = 'https://i.taobao.com/my_taobao.htm'

r = requests.get(url=url, headers=headers)

print(r.content.decode())
```

也可以对cookies以参数形式传递，cookies为字典



```python
r = requests.get('http://xxxx',headers=headers, cookies=cookies)
```

- cookie过期时间很长的
- 在cookie过期之前能够拿到所有的数据，比较麻烦
- 配合其他程序一起使用，其他程序专门获取cookie，当前程序专门请求页面

**`寻找登录的post地址`**

- 在form表单中查找actiond的url地址
  - post的数据是input标签中的name的值作为键，真正的用户名密码作为值的字典，post的url地址就是action对应的url地址
- 抓包，看看当我们提交的时候，究竟给哪个网址发送了post请求
  - 勾选perserve log按钮，防止页面跳转找不到url
  - 寻找post数据，确定参数
    - 参数不会变：（如何确定参数会不会变？多请求几次），直接用，比如密码不是动态加密的时候
    - 参数会变
      - 参数在当前的响应中
      - 通过js生成：定位到对应的js查看

**`定位想要的js`**

- 法一：对于Chrome浏览器

  - 选择登录按钮（或任意绑定了js事件的对象）
  - Eventlistener
  - 勾选Framework listeners
  - 查看对应的js
  - 找到登录按钮对应的函数
  - （如果遇到某个元素（如：$('.password').value）是干嘛的，可以copy到console中去进行查看；也可以直接对js添加断点）

- 法二：对于Chrome浏览器

  - 直接通过Chrome中的search all file的搜索url中的关键字

- 法三

  添加断点的方式来查看js的操作，通过python进行同样的操作，就可以得到js生成的数据


# 3. Request模块入门

> Q： request和urllib的区别：   
>
> 1. requests的底层实现是就urllib，urllib能做的事情，requests都可以做；    
> 2. requests在python2和python3中通用，方法完全一样；    
> 3. requests简单易用；    
> 4. request能够自动帮我们解压（gzip等）网页内容  

 `中文文档api：http://docs.python-requests.org/zh_CN/latest/index.html`

**基础使用**

```python
"""基础入门"""
import requests

r = requests.get('http://www.baidu.com')    #r即为获得的响应。所请求的所有东西都在r中
# 必须要包含协议（http或https）；还有dlelete，post等方法

print(r)

print(r.text)   #text是一个属性，其实可以通过他的意思判断，text是一个名字，所以是属性，如果是方法，  		         常为动词
                #会自动根据推测的编码方式解码为str

print(r.encoding)   #根据头部推测编码方式，如果猜错了，我们解码的时候就必须自己指定解码的方式

print(r.content)    #也是属性，是一个bytes类型的数据。
print(r.content.decode())   #将bytes类型转换为str类型。默认的解码方式为utf-8
print(r.status_code)    #获取状态码
assert r.status_code == 200   #断言状态码为200，如果断言失败，会报错：assertionerror
#可以用此方法判断请求是否成功

print(r.headers)    #响应头，我们主要关注其中的set-cookie（在本地设置cookie）字段
print(r.request)    #关于对应相应的请求部分，是一个对象
print(r.request.url)    #请求的url地址
print(r.url)    #响应地址，请求的url地址和响应的url地址可能会不一样（因为可能和重定向）
print(r.request.headers)    #请求头，如果不设置，默认的user-agent是python-requests/x.xx.x

with open('baidu_r.txt','w') as f:    #测试：查看默认的user-agent访问时返回的内容
    f.write(r.content.decode())

"""
requests中解编码的方法：
1. r.content.decode()   #content的类型为bytes，必须再次解码
2. r.content.decode('gbk')
3. r.text       #text是按照推测的编码方式进行解码后的数据，他的类型为str
"""
```

**发送带header的请求**

具体的header到浏览器中进行查看

```python
"""
为什么请求需要带上header？
    模拟浏览器，欺骗服务器，获取和浏览器一致的内容

header的形式：字典，形式：{request headers冒号前面的值:request headers冒号后面的值}，大部分情况，我们带上user-agent即可，少数情况需要cookie
用法：requests.get(url,headers=headers) 
"""

import requests

headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:68.0) Gecko/20100101 Firefox/68.0'}

response = requests.get('https://www.baidu.com',headers=headers)

print(response.content.decode())    #会发现响应中的数据比不带header多许多
```

**发送带参数的请求**

```python
#在url中带参数的形式
#例如：在我们百度搜索某东西时，就会带上一大堆参数，但是大部分可能是没有用的，我们可以尝试删除，然后我们在爬虫中带的参数只需要为其中不能删除的部分即可
"""
参数的形式：字典
kw={'wd':'长城'}   搜索的内容可以在浏览器地址栏看到，&wd=搜索的内容&
用法：requests.get(url,params=kw)
"""
import requests

headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:68.0) Gecko/20100101 Firefox/68.0'}

params = {'wd':'这里是中文'}
#如果参数含中文，其就会被自动编码，编码的后的形式含百分号，我们可以使用url解码来查看原来的内容

r = requests.get('https://www.baidu.com',params=params,headers=headers)
print(r.status_code)
print(r.request.url)
print(r.url)
print(r.content.decode())

#当然，我们也可以直接把参数拼接到url中去，而不单独传参(也不需要手动编码)，
eg：r = requests.get('https://www.baidu.com/s?wd={}'.formate('传智播客'))
```

**小练习：爬贴吧前1000页**

```python
import requests
kw = input('请输入您要爬取的贴吧名：')
url = 'https://tieba.baidu.com/f?kw=%{kw}8&pn='.format(kw=kw)
headers = {'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:68.0) Gecko/20100101 Firefox/68.0'}


for i in range(1000):
    url = urlr.formate(str(i*50))
    r = requests.get(url=url, headers=headers)
    with open('./tieba_pages/{}-第{}页.html'.format(kw,i), 'w', encoding='utf-8') as f:
        # 为什么是utf-8，因为r.content.decode()为utf-8的格式
        f.write(r.content.decode())
```

`扁平胜于嵌套`：比如，多用列表推倒式替代某些循环
## Requests的小技巧

**cookie对象与字典的相互转化与url编解码**

```python
"""1. 把cookie对象（cookiejar）转化为字典"""
import requests
r = requests.get('http://www.baidu.com')
print(r.cookies)
ret = requests.utils.dict_from_cookiejar(r.cookies)
print(ret)  #输出：{'BDORZ': '27315'}
"""将字典转化为cookiejar"""
print(requests.utils.cookiejar_from_dict(ret))

"""2. url地址的编解码"""
url = 'http://www.baidu.com'
print(requests.utils.quote(url))    #输出：http%3A//www.baidu.com
print(requests.utils.unquote(requests.utils.quote(url)))    #输出：http://www.baidu.com

"""输出结果如下：
<RequestsCookieJar[<Cookie BDORZ=27315 for .baidu.com/>]>
{'BDORZ': '27315'}
<RequestsCookieJar[<Cookie BDORZ=27315 for />]>
http%3A//www.baidu.com
http://www.baidu.com
"""
```

**请求SSL证书验证与超时设置**

如果某网站使用的是https，但是又没有购买ssl证书，等浏览器访问时就会提示不安全，而当我们使用爬虫爬取的就会报错，此时，我们可以用verify=False来解决



```python
import requests
r = requests.get('https://www.12306.cn/mormhweb/',verify=False, timeout=10) #如果超时，会报错，因此要结合try使用
"""注意：此时不会报错，但是会warning"""
```

**配合状态码判断是否请求成功**



```python
assert response.status_code == 200  #如果断言失败，会报错，因此应该结合try使用
```

**重新请求**

使用retrying模块，通过装饰器的方式使用



```python
"""重新请求"""
import requests
from retrying import retry

headers= {
            'User-Agent':'Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1'
        }
@retry(stop_max_attempt_number=3)   #即下面的方法最多可以报错3次，如果3次都报错，则程序报错
def _parse_url(url):
    r = requests.get(url,headers=headers,timeout=0.01)
    assert r.status_code == 200
    return r.content.decode()

def parse_url(url):
    try:
        html_str = _parse_url(url)
    except:
        html_str = None
    return html_str
if __name__ == "__main__":
    url = 'http://www.baidu.com'
    print(parse_url(url))
```

ps：安装第三方模块的方法

- pip install
- 下载源码文件，进入解压后的目录：`python setup.py install` 
-  `xxx.whl`文件，安装方法：pip install xxx.whl
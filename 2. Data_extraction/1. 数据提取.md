# 数据提取

## 1. 什么是数据提取？

> 从响应中获取我们想要的数据

## 2. 数据分类

- 非结构话数据：html等
  - 处理方法：正则表达式、xpath
- 结构化数据：json、xml等
  - 处理方法：转化为python数据类型

   **主要是看结构清不清晰**

## 数据提取之JSON

由于把json数据转化为python内置数据类型很简单，所以爬虫中，我们常（优先）**使用能够返回json数据的url**

JSON（JavaScript Object Notation）是一种轻量级的**数据交换格式**，它使得人们很容易进行阅读和编写 。同时也方便了机器进行解析和生成，适用于**进行数据交换的场景**，比如网站前后台间的数据交换

Q：哪里能够找到返回json的url呢？

- 使用chrome切换到手机页面
- 抓包手机app的软件

![1644730915269](C:\Users\LDZ\AppData\Roaming\Typora\typora-user-images\1644730915269.png)

**json.loads与json.dumps**

```python
"""
1. json.loads   能够把json字符串转换成python类型
2. json.dumps   能够把python类型转换为json字符串，当我们把数据保存问文本的时候常常需要这么做，如果要使其显示中文，可以使用参数：ensure_ascii=False；还使用使用参数：indent=2，使下一级相对上一级有两个空格的缩进
"""
```

使用json的注意点：

- json中的引号都是双引号；
  - 如果不是双引号
    - eval：能实现简单的字符串和python类型的转化
    - replace：把单引号替换为双引号
- 往一个文件中写如多个json串，不再是一个json串
  - 可以一行写一个json串，按照行来读取

**json.load与json.dump**

类文件对象：具有read和write方法的对象就是类文件对象，比如：f = open('a.txt','r')，f就是类文件对象（fp）

```python
"""
1. json.load(类文件对象) #类型为dict
2. json.dump(python类型, 类文件对象)   #把python类型放入类文件对象中，也可以使用ensure_ascii和indent参数
"""
```

json在数据交换中起到了一个载体的作用，承载着相互传递的数据

![1644731025147](C:\Users\LDZ\AppData\Roaming\Typora\typora-user-images\1644731025147.png)

案例：爬取豆瓣

```python
import requests
from pprint import pprint   #pprint：pretty print，实现美化输出
import json

from retrying import retry

url = 'https://m.douban.com/rexxar/api/v2/skynet/playlists?from_rexxar=true&for_mobile=1'

headers = {
    'User-Agent': 'Mozilla/5.0 (Linux; Android 8.0; Pixel 2 Build/OPD3.170816.012) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Mobile Safari/537.36',
    # 'Sec-Fetch-Mode': 'cors'
    'Referer': 'https://m.douban.com/movie/beta'
    #在本次爬取过程中，必须加上Referer才行
}
@retry(stop_max_attempt_number=3)
def parse_url(url):
    r = requests.get(url,headers=headers, timeout=10)
    assert r.status_code == 200
    return r.content.decode()
    
resp_html = parse_url(url)
p_resp = json.loads(resp_html)
pprint(p_resp)
with open('douban.json','w', encoding='utf-8') as f:
    f.write(json.dumps(p_resp, indent=2, ensure_ascii=False))
```



```python
"""爬取36kr"""
import requests,json
from pprint import pprint
import re
url = 'https://36kr.com/'

headers = {
    'User-Agent':'Mozilla/5.0 (Linux; Android 8.0; Pixel 2 Build/OPD3.170816.012) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Mobile Safari/537.36'
}
r = requests.get(url=url, headers=headers,timeout=3)
html_str = r.content.decode()
reg = '<span class="item-title weight-bold ellipsis-2">(.*?)</span>'    #新闻的标题是直接在html中的
ret = re.findall(reg, html_str)
pprint(ret)
```

**`爬虫思路总结`**

- 通常，我们访问某个网站时，得到的是其主页的url
- 得到了主页的url后，观察我们所需要的数据是否在主页对应的响应中，如果在，直接利用主页的url爬取
- 如果不在主页的url中，查找我们需要的数据，得到其对应url，用该url进行数据的爬取
- 如果相应数据不是在html中，而是json中，用json.loads对数据进行处理

